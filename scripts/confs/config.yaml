defaults:
  - _self_
  - optimizer: adam
  - lr_scheduler: step_lr
  - dataset: constantcount_sad
  #constantcount_h3m constantcount_sad constantcount_dhp19
  - loss: multipixelwise
  - augmentation: no_aug
  - training: margipose
  # sadpose margipose
  - override hydra/help: help

# hydra: standard
gpus: [0]
epochs: 100
early_stopping: 10
batch_size: 16
#32
num_workers: 0
#16
train_val_split: 0.8
train_shuffle: false

debug: false
root_root: /home/miracle/kirito/aico/datasets/DVS
root_dir: ${root_root}/mmd/experiments
# J:\datasets\DVS\mmd\experiments
# D:\Dataset\DVS\dhp19\experiments
# /mnt/d/Dataset/DVS/dhp19/experiments
# /data/gscarpellini/exps
model_zoo: ${root_root}/dhp19/pretrained_model
# J:\datasets\DVS\dhp19\pretrained_model
# /mnt/d/Dataset/DVS/dhp19/pretrained_model
#/data/gscarpellini/model_zoo
project_name: event_hpe
resume: false
result_file: results.json 
load_path:
accelerator: ddp

exp_dir: exps_${training.module}/${dataset.partition}
exp_name: ${now:%m-%d-%H-%M}_exp_${training.model}_pretrained_${training.pretrained}

hydra:
  output_subdir: reproduce
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '_'
        item_sep: '__'
        exclude_keys: []
  sweep:
    dir: ${root_dir}/exps/${dataset.name}/${exp_dir}
    subdir: ${exp_name}
    
  run:
    dir: ${hydra.sweep.dir}/${hydra.sweep.subdir}
      